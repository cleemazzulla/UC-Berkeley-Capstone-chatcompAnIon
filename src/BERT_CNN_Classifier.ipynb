{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64cf24c9-0a73-4779-ac53-0dee5b9956d3",
   "metadata": {},
   "source": [
    "### 1. Setting up the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb1c3e4b-423b-4c57-8018-a865e2be231e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.11/site-packages/pyspark/pandas/__init__.py:50: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/ubuntu/spark-3.5.1-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/ubuntu/.ivy2/cache\n",
      "The jars for the packages stored in: /home/ubuntu/.ivy2/jars\n",
      "com.databricks#spark-xml_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-1d4518f9-895b-41cc-95d1-a76c138f733e;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.databricks#spark-xml_2.12;0.17.0 in central\n",
      "\tfound commons-io#commons-io;2.11.0 in central\n",
      "\tfound org.glassfish.jaxb#txw2;3.0.2 in central\n",
      "\tfound org.apache.ws.xmlschema#xmlschema-core;2.3.0 in central\n",
      "\tfound org.scala-lang.modules#scala-collection-compat_2.12;2.9.0 in central\n",
      ":: resolution report :: resolve 261ms :: artifacts dl 21ms\n",
      "\t:: modules in use:\n",
      "\tcom.databricks#spark-xml_2.12;0.17.0 from central in [default]\n",
      "\tcommons-io#commons-io;2.11.0 from central in [default]\n",
      "\torg.apache.ws.xmlschema#xmlschema-core;2.3.0 from central in [default]\n",
      "\torg.glassfish.jaxb#txw2;3.0.2 from central in [default]\n",
      "\torg.scala-lang.modules#scala-collection-compat_2.12;2.9.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   5   |   0   |   0   |   0   ||   5   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-1d4518f9-895b-41cc-95d1-a76c138f733e\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 5 already retrieved (0kB/16ms)\n",
      "24/03/16 18:01:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# Install Java\n",
    "#!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "\n",
    "# Download Spark\n",
    "#if os.path.isfile(\"./spark-3.5.1-bin-hadoop3.tgz\") == False:\n",
    "#    !wget -q https://downloads.apache.org/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3.tgz\n",
    "\n",
    "# Unzip the file\n",
    "#!tar xf spark-3.5.1-bin-hadoop3.tgz\n",
    "\n",
    "# Setup environment for Spark\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = '/home/ubuntu/spark-3.5.1-bin-hadoop3'\n",
    "\n",
    "# Import findspark and load it\n",
    "#!pip install -q findspark\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# Install spark-nlp\n",
    "#!pip install spark-nlp\n",
    "import sparknlp\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.common import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler, Imputer, OneHotEncoder, StringIndexer\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator, CrossValidatorModel, TrainValidationSplit, TrainValidationSplitModel\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "import pyspark.pandas as ps\n",
    "\n",
    "# Create Spark session\n",
    "#from pyspark.sql import SparkSession\n",
    "# spark = SparkSession.builder\\\n",
    "#        .master(\"local\")\\\n",
    "#        .appName(\"Spark NLP\")\\\n",
    "#        .config('spark.executor.memory', \"24g\")\\\n",
    "#        .getOrCreate()\n",
    "\n",
    "# Start Spark Session with Spark NLP\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "                    .master(\"local[*]\") \\\n",
    "                    .config(\"spark.jars.packages\", \"com.databricks:spark-xml_2.12:0.17.0\") \\\n",
    "                    .appName(\"MyApp\") \\\n",
    "                    .getOrCreate()\n",
    "# spark = sparknlp.start(gpu=False)\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20f6a562-ac04-48d6-ab65-18dde7985d9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-16-34.us-west-2.compute.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>MyApp</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f73541d7f50>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3df18b8-db6c-4ba1-9d58-ae5e6fae15c9",
   "metadata": {},
   "source": [
    "### 2. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac09f93b-8fe3-4890-9731-d31eb25fb7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import pickle\n",
    "import urllib.request\n",
    "# import altair as alt\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, fbeta_score\n",
    "import collections\n",
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from matplotlib import pyplot\n",
    "from numpy import where\n",
    "import imblearn\n",
    "\n",
    "# visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image, display, HTML\n",
    "import pydot\n",
    "from pydotplus import graph_from_dot_data\n",
    "from sklearn.tree import export_graphviz\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "from xml.etree.ElementTree import ElementTree\n",
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, fbeta_score\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d469f6e6-c54b-4a2a-9080-0e862047f4bb",
   "metadata": {},
   "source": [
    "### 3. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7389b02f-262d-4a92-afd5-97ff6aa2be15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/16 18:01:30 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_merged_data_conversations_path = 's3a://capstone210/data/train_merged_data_conversations/'\n",
    "test_merged_data_conversations_path = 'file:/home/ubuntu/workspace/capstone-210-spring2024/data/test'\n",
    "\n",
    "train_merged_data_conversations_df = spark.read.parquet(train_merged_data_conversations_path)\n",
    "test_merged_data_conversations_df = spark.read.parquet(test_merged_data_conversations_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d98f730-0ad7-4c1d-9c25-a221cc8af38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_final = train_merged_data_conversations_df\n",
    "test_df_final = test_merged_data_conversations_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9902ed89-eae7-4139-a3b8-8606389ed7ee",
   "metadata": {},
   "source": [
    "### 4. Split the data and implement different sampling method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693ede7a-b91d-4547-a242-f35b4bb05225",
   "metadata": {},
   "source": [
    "#### 4.1 Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f1dfe78-6efc-4c9d-a222-6a33fe4bfc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to undersample dataset automatically\n",
    "def undersample(df, outcome_col, seed=1234):\n",
    "  # Split dataset based on outcome\n",
    "  split0_df = df.filter(col(outcome_col) == 0)\n",
    "  split1_df = df.filter(col(outcome_col) == 1)\n",
    "  # determine which split is major vs minor\n",
    "  if (split0_df.count() > split1_df.count()):\n",
    "    major_df = split0_df\n",
    "    minor_df = split1_df\n",
    "  else:\n",
    "    minor_df = split0_df\n",
    "    major_df = split1_df\n",
    "  ratio = major_df.count()/minor_df.count()\n",
    "  print(\"Ratio of major vs minor before sampling: {}\".format(ratio))\n",
    "  # Start under-sampling with Spark\n",
    "  sampled_majority_df = major_df.sample(False, 1/ratio, seed)\n",
    "  combined_df = sampled_majority_df.unionAll(minor_df)\n",
    "  print(f\"Final sample size: {combined_df.count()}\")\n",
    "  return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8b9c278-77da-481d-ae8f-5a7841be1909",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio of major vs minor before sampling: 33.05456656346749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final sample size: 5239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Perform undersampling technique\n",
    "df_train_UnderSampled = undersample(train_df_final, outcome_col='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5613877-fe46-4328-859b-65f4af9169c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the split ratios (80% train, 20% validation)\n",
    "train_ratio = 0.7\n",
    "validation_ratio = 0.3\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "df_train_split, df_val_split = df_train_UnderSampled.randomSplit([train_ratio, validation_ratio], seed=42)\n",
    "\n",
    "# naming convention\n",
    "df_test_split = test_df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a8a4349-1e5f-4cff-808d-fc0affb95e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Convert PySpark DataFrame to Pandas DataFrame\n",
    "df_train_pd = df_train_split.select('merged_text', 'type_conversation', 'label').toPandas()\n",
    "df_val_pd = df_val_split.select('merged_text', 'type_conversation', 'label').toPandas()\n",
    "df_test_pd = df_test_split.select('merged_text', 'type_conversation', 'label').toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f92b55-ed83-49b5-b00d-7abeb5291017",
   "metadata": {},
   "source": [
    "#### 4.2 Nearest neighbor sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa37c821-677b-4edd-84ad-93a2a1718c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def nearest_neighbor_sampling(data, k=2):\n",
    "#     \"\"\"\n",
    "#     Perform Nearest Neighbor Sampling (NNS) for a dataset.\n",
    "\n",
    "#     Parameters:\n",
    "#     - data: DataFrame or array-like, shape (n_samples, n_features)\n",
    "#             The input dataset.\n",
    "#     - k: int, optional (default=2)\n",
    "#          Number of nearest neighbors to consider for sampling.\n",
    "\n",
    "#     Returns:\n",
    "#     - sampled_data: DataFrame or array-like, shape (n_samples, n_features)\n",
    "#                     The sampled dataset.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Initialize Nearest Neighbors model\n",
    "#     nn_model = NearestNeighbors(n_neighbors=k)\n",
    "#     nn_model.fit(data)\n",
    "\n",
    "#     # Find nearest neighbors\n",
    "#     indices = nn_model.kneighbors(data, return_distance=False)[:, 1]\n",
    "\n",
    "#     # Sample data based on nearest neighbors\n",
    "#     sampled_data = data.iloc[indices]\n",
    "\n",
    "#     return sampled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a38f6c4c-2149-4fe6-83c1-b1fdba17192f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extract the features you want to sample from the DataFrame\n",
    "# features_to_sample = train_df_final[['n_people_in_conversation', 'label']]\n",
    "\n",
    "# # Convert the DataFrame to a 2D array\n",
    "# # features_array = features_to_sample.values\n",
    "\n",
    "# # Apply Nearest Neighbor Sampling\n",
    "# df_train_NNS = nearest_neighbor_sampling(features_to_sample, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6d39076-3cb8-4712-ac90-7dd8f792f384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(input_ids, df['label'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de8e6c63-5c67-4235-9b0b-b57bbf9c8b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert PySpark DataFrame to Pandas DataFrame\n",
    "# df_train_pd = df_train_split.select('merged_text', 'label').toPandas()\n",
    "# df_val_pd = df_val_split.select('merged_text', 'label').toPandas()\n",
    "# df_test_pd = df_test_split.select('merged_text', 'label').toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee66835d-27a5-4d5e-bc43-1a6ad920f7e6",
   "metadata": {},
   "source": [
    "### 5. Build the model pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bb4c7cb2-51d0-403e-9393-19be10d70af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ca72c593-094c-4c36-a443-64efc65a95e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize type_conversation\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize input data\n",
    "type_conv_tokens = tokenizer.batch_encode_plus(df_train_pd['type_conversation'].tolist(), padding=True, truncation=True, max_length=16, return_tensors='tf')\n",
    "convo_tokens = tokenizer.batch_encode_plus(df_train_pd['merged_text'].tolist(), padding=True, truncation=True, max_length=128, return_tensors='tf')\n",
    "\n",
    "# # Tokenize text data\n",
    "# text_tokens = tokenizer(df['merged_text'].tolist(), padding='max_length', truncation=True, max_length=max_length, return_tensors='tf')\n",
    "# time_tokens = tokenizer(df['conversation_start_time'].astype(str).tolist(), padding='max_length', truncation=True, max_length=max_length, return_tensors='tf')\n",
    "# type_tokens = tokenizer(df['type_conversation'].astype(str).tolist(), padding='max_length', truncation=True, max_length=max_length, return_tensors='tf')\n",
    "\n",
    "# Merge tokenized inputs\n",
    "# input_ids = tf.concat([text_tokens['input_ids'], time_tokens['input_ids'], type_tokens['input_ids']], axis=1)\n",
    "# attention_masks = tf.concat([text_tokens['attention_mask'], time_tokens['attention_mask'], type_tokens['attention_mask']], axis=1)\n",
    "# input_ids = tf.concat([convo_tokens['input_ids'], type_conv_tokens['input_ids']], axis=1)\n",
    "# attention_masks = tf.concat([convo_tokens['attention_mask'], type_conv_tokens['attention_mask']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93445db9-536a-49d4-b259-4b5b775f3a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tokenize merged_text, conversation_start_time, and type_conversation\n",
    "# def tokenize_data(df):\n",
    "#     inputs = tokenizer(df['merged_text'].tolist(), \n",
    "#                        df['type_conversation'].tolist(),\n",
    "#                        padding=True,\n",
    "#                        truncation='longest_first',\n",
    "#                        max_length=512,\n",
    "#                        return_tensors='pt')\n",
    "#     return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e49385d9-29a1-4145-9b4d-33c7498a8c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# warnings.filterwarnings(\"ignore\", message=\"Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1cf8caa7-70c8-4b8a-ac6a-884a7b5a6a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_inputs = tokenize_data(df_train_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a08d6bb8-2628-43da-ab74-b2d4e0158bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_inputs = tokenize_data(df_val_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "541dcbee-9257-431b-9dad-fd017053c2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_inputs = tokenize_data(df_test_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0dff7a3f-468b-492c-ae62-467d94cb3290",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTCNNEnsemble(nn.Module):\n",
    "    def __init__(self, bert_model, cnn_model, num_labels):\n",
    "        super(BERTCNNEnsemble, self).__init__()\n",
    "        self.bert_model = bert_model\n",
    "        self.cnn_model = cnn_model\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(768 + cnn_model.output_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, type_conv_ids):\n",
    "        # BERT Model\n",
    "        outputs = self.bert_model(input_ids, attention_mask, token_type_ids)\n",
    "        pooled_output = outputs[1]  # Use pooled output (CLS token)\n",
    "        \n",
    "        # CNN Model\n",
    "        cnn_output = self.cnn_model(type_conv_ids)\n",
    "\n",
    "        # Concatenate and classify\n",
    "        concat_output = torch.cat((pooled_output, cnn_output), dim=1)\n",
    "        concat_output = self.dropout(concat_output)\n",
    "        logits = self.fc(concat_output)\n",
    "        return logits\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, kernel_sizes, output_size):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.convs = nn.ModuleList([nn.Conv1d(embedding_dim, output_size, kernel_size) for kernel_size in kernel_sizes])\n",
    "        self.output_size = output_size * len(kernel_sizes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # (batch_size, seq_len, embedding_dim)\n",
    "        x = x.transpose(1, 2)  # (batch_size, embedding_dim, seq_len)\n",
    "        x = [conv(x) for conv in self.convs]\n",
    "        x = [torch.max_pool1d(conv, conv.size(-1)).squeeze(-1) for conv in x]\n",
    "        x = torch.cat(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c26e4960-7413-43a0-90d8-8648be3f0a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([  101 18847 24277   102], shape=(4,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(type_conv_tokens['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4e1a0663-559c-4679-b723-b9deb0f9959d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m ensemble_model \u001b[38;5;241m=\u001b[39m BERTCNNEnsemble(bert_model, cnn_model, num_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m logits \u001b[38;5;241m=\u001b[39m ensemble_model(input_ids, attention_mask, type_convo_ids, type_convo_att)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[35], line 11\u001b[0m, in \u001b[0;36mBERTCNNEnsemble.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, type_conv_ids)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, attention_mask, token_type_ids, type_conv_ids):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# BERT Model\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbert_model(input_ids, attention_mask, token_type_ids)\n\u001b[1;32m     12\u001b[0m     pooled_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# Use pooled output (CLS token)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# CNN Model\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:961\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    959\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    960\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarn_if_padding_and_no_attention_mask(input_ids, attention_mask)\n\u001b[0;32m--> 961\u001b[0m     input_shape \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m    962\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    963\u001b[0m     input_shape \u001b[38;5;241m=\u001b[39m inputs_embeds\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not callable"
     ]
    }
   ],
   "source": [
    "# Create input tensors\n",
    "# input_ids = train_inputs['input_ids']\n",
    "# attention_mask = train_inputs['attention_mask']\n",
    "# token_type_ids = train_inputs['token_type_ids']\n",
    "# type_conv_ids = type_conv_tokens['input_ids']\n",
    "# input_ids = convo_tokens['input_ids']\n",
    "# attention_mask = convo_tokens['attention_mask']\n",
    "# token_type_ids = type_conv_tokens['token_type_ids']\n",
    "# type_conv_ids = type_conv_tokens['input_ids']\n",
    "\n",
    "input_ids = convo_tokens['input_ids'].numpy()\n",
    "attention_mask = convo_tokens['attention_mask'].numpy()\n",
    "type_convo_ids = type_conv_tokens['input_ids'].numpy()\n",
    "type_convo_att = type_conv_tokens['attention_mask'].numpy()\n",
    "\n",
    "# Initialize BERT and CNN models\n",
    "cnn_model = CNNModel(vocab_size=tokenizer.vocab_size, embedding_dim=128, kernel_sizes=[2, 3, 4], output_size=64)\n",
    "\n",
    "# Initialize the ensemble model\n",
    "ensemble_model = BERTCNNEnsemble(bert_model, cnn_model, num_labels=2)\n",
    "\n",
    "# Forward pass\n",
    "logits = ensemble_model(input_ids, attention_mask, type_convo_ids, type_convo_att)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41030ab5-4c65-434f-858c-2c8df7efe5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_model.eval()\n",
    "val_preds = []\n",
    "val_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        input_ids, attention_mask, token_type_ids, type_conv_ids, labels = batch\n",
    "        logits = ensemble_model(input_ids, attention_mask, token_type_ids, type_conv_ids)\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        val_preds.extend(predicted.tolist())\n",
    "        val_targets.extend(labels.tolist())\n",
    "\n",
    "# Calculate performance scores\n",
    "precision = precision_score(val_targets, val_preds)\n",
    "recall = recall_score(val_targets, val_preds)\n",
    "f1 = f1_score(val_targets, val_preds)\n",
    "f_beta = fbeta_score(val_targets, val_preds, beta=0.5)\n",
    "\n",
    "print(\"Validation Performance Scores:\")\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"F1 Beta Score:\", f_beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6835e5e3-c6e9-4716-9e9e-1a36c6242a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define BERT-CNN ensemble model\n",
    "class BERTCNNEnsemble(nn.Module):\n",
    "    def __init__(self, bert_model, cnn_model):\n",
    "        super(BERTCNNEnsemble, self).__init__()\n",
    "        self.bert_model = bert_model\n",
    "        self.cnn_model = cnn_model\n",
    "        self.fc = nn.Linear(384, 2)  # Adjust input size based on the output of the CNN model\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        # BERT Model\n",
    "        outputs = self.bert_model(input_ids, attention_mask, token_type_ids)\n",
    "        pooled_output = outputs[1]  # Use pooled output (CLS token)\n",
    "\n",
    "        # CNN Model\n",
    "        cnn_output = self.cnn_model(pooled_output.unsqueeze(0))\n",
    "\n",
    "        # Concatenate and classify\n",
    "        logits = self.fc(cnn_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b64a96d5-b74e-4a20-94c9-327182e83606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c2a4990-c2bb-4cfb-9e3a-e8f518fe9dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33e2c114-d9ad-4ff8-9e3a-79353c869059",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Conv1D, GlobalMaxPooling1D, Concatenate, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36af6ca1-05f6-4e50-bc4c-295325ad5e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CNN model architecture using PyTorch\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(input_size, 128, kernel_size=3)\n",
    "        self.conv2 = nn.Conv1d(input_size, 128, kernel_size=4)\n",
    "        self.conv3 = nn.Conv1d(input_size, 128, kernel_size=5)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool1d(kernel_size=3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.pool(self.relu(self.conv1(x)))\n",
    "        x2 = self.pool(self.relu(self.conv2(x)))\n",
    "        x3 = self.pool(self.relu(self.conv3(x)))\n",
    "        return torch.cat((x1, x2, x3), dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "03329668-a70f-456d-a5ee-c7efbd98465c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training parameters\n",
    "batch_size = 32\n",
    "learning_rate = 1e-5\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b1c4366-7359-4d4f-8576-363c14f6f3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a6c47c14-3947-4013-ae7e-ae0982170af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, inputs, labels):\n",
    "        self.inputs = inputs\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs['input_ids'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.inputs['input_ids'][idx]\n",
    "        attention_mask = self.inputs['attention_mask'][idx]\n",
    "        token_type_ids = self.inputs['token_type_ids'][idx]\n",
    "        label = torch.tensor(self.labels[idx])\n",
    "        return input_ids, attention_mask, token_type_ids, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "44072bc6-cefe-40ea-80de-c45decd54fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "train_dataset = CustomDataset(train_inputs, df_train_pd['label'].tolist())\n",
    "# val_dataset = CustomDataset(val_inputs, df_val_pd['label'].tolist())\n",
    "# test_dataset = CustomDataset(test_inputs, df_test_pd['label'].tolist())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "edf000a1-f346-4aec-853e-fc199cf5c15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate CNN model\n",
    "cnn_model = CNNModel(input_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a7d5e306-612b-4720-96ed-e9abdc6d1caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate BERT-CNN ensemble model\n",
    "ensemble_model = BERTCNNEnsemble(bert_model, cnn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "206597f0-1f79-48c3-8194-be6e2b6e11f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(ensemble_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0adc8b48-9041-43fe-82d4-99ba58770e32",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [128, 128, 3], expected input[1, 32, 768] to have 128 channels, but got 32 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m input_ids, attention_mask, token_type_ids, labels \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m      6\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m----> 7\u001b[0m outputs \u001b[38;5;241m=\u001b[39m ensemble_model(input_ids, attention_mask, token_type_ids)\n\u001b[1;32m      8\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m      9\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[14], line 15\u001b[0m, in \u001b[0;36mBERTCNNEnsemble.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids)\u001b[0m\n\u001b[1;32m     12\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# Use pooled output (CLS token)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# CNN Model\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m cnn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcnn_model(pooled_output\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Concatenate and classify\u001b[39;00m\n\u001b[1;32m     18\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(cnn_output)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[16], line 12\u001b[0m, in \u001b[0;36mCNNModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 12\u001b[0m     x1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x)))\n\u001b[1;32m     13\u001b[0m     x2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)))\n\u001b[1;32m     14\u001b[0m     x3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3(x)))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:310\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:306\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    304\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    305\u001b[0m                     _single(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 306\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(\u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    307\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [128, 128, 3], expected input[1, 32, 768] to have 128 channels, but got 32 channels instead"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    ensemble_model.train()\n",
    "    for batch in train_loader:\n",
    "        input_ids, attention_mask, token_type_ids, labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        outputs = ensemble_model(input_ids, attention_mask, token_type_ids)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bc7ae3-a2ca-4d8e-8a23-632f57d89fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation loop\n",
    "ensemble_model.eval()\n",
    "val_preds = []\n",
    "val_targets = []\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        input_ids, attention_mask, token_type_ids, labels = batch\n",
    "        logits = ensemble_model(input_ids, attention_mask, token_type_ids)\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        val_preds.extend(predicted.tolist())\n",
    "        val_targets.extend(labels.tolist())\n",
    "\n",
    "# Calculate performance scores\n",
    "precision = precision_score(val_targets, val_preds)\n",
    "recall = recall_score(val_targets, val_preds)\n",
    "f1 = f1_score(val_targets, val_preds)\n",
    "f_beta = fbeta_score(val_targets, val_preds, beta=0.5)\n",
    "\n",
    "print(\"Validation Performance Scores:\")\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"F1 Beta Score:\", f_beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a956fb-9dbf-4074-857d-154f1766b221",
   "metadata": {},
   "source": [
    "#### Build ML Pipeline with pretrained BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ae4f6f3-1670-4388-938e-be71067c16a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'JavaPackage' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Step 1: Transforms raw texts to `document` annotation\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m document \u001b[38;5;241m=\u001b[39m DocumentAssembler()\\\n\u001b[1;32m      3\u001b[0m               \u001b[38;5;241m.\u001b[39msetInputCol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmerged_text\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n\u001b[1;32m      4\u001b[0m               \u001b[38;5;241m.\u001b[39msetOutputCol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocument\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n\u001b[1;32m      5\u001b[0m               \u001b[38;5;241m.\u001b[39msetCleanupMode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshrink\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Step 2: Encodes text into high dimensional vectors\u001b[39;00m\n\u001b[1;32m      8\u001b[0m bert_sent \u001b[38;5;241m=\u001b[39m BertSentenceEmbeddings\u001b[38;5;241m.\u001b[39mpretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msent_small_bert_L8_512\u001b[39m\u001b[38;5;124m'\u001b[39m)\\\n\u001b[1;32m      9\u001b[0m               \u001b[38;5;241m.\u001b[39msetInputCols([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocument\u001b[39m\u001b[38;5;124m\"\u001b[39m])\\\n\u001b[1;32m     10\u001b[0m               \u001b[38;5;241m.\u001b[39msetOutputCol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pyspark/__init__.py:139\u001b[0m, in \u001b[0;36mkeyword_only.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMethod \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m forces keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sparknlp/base/document_assembler.py:96\u001b[0m, in \u001b[0;36mDocumentAssembler.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;129m@keyword_only\u001b[39m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28msuper\u001b[39m(DocumentAssembler, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(classname\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcom.johnsnowlabs.nlp.DocumentAssembler\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setDefault(outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocument\u001b[39m\u001b[38;5;124m\"\u001b[39m, cleanupMode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisabled\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pyspark/__init__.py:139\u001b[0m, in \u001b[0;36mkeyword_only.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMethod \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m forces keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sparknlp/internal/annotator_transformer.py:36\u001b[0m, in \u001b[0;36mAnnotatorTransformer.__init__\u001b[0;34m(self, classname)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msetParams(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m_java_class_name \u001b[38;5;241m=\u001b[39m classname\n\u001b[0;32m---> 36\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_java_obj(classname, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muid)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pyspark/ml/wrapper.py:86\u001b[0m, in \u001b[0;36mJavaWrapper._new_java_obj\u001b[0;34m(java_class, *args)\u001b[0m\n\u001b[1;32m     84\u001b[0m     java_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(java_obj, name)\n\u001b[1;32m     85\u001b[0m java_args \u001b[38;5;241m=\u001b[39m [_py2java(sc, arg) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args]\n\u001b[0;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m java_obj(\u001b[38;5;241m*\u001b[39mjava_args)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'JavaPackage' object is not callable"
     ]
    }
   ],
   "source": [
    "# Step 1: Transforms raw texts to `document` annotation\n",
    "document = DocumentAssembler()\\\n",
    "              .setInputCol(\"merged_text\")\\\n",
    "              .setOutputCol(\"document\")\\\n",
    "              .setCleanupMode(\"shrink\")\n",
    "\n",
    "# Step 2: Encodes text into high dimensional vectors\n",
    "bert_sent = BertSentenceEmbeddings.pretrained('sent_small_bert_L8_512')\\\n",
    "              .setInputCols([\"document\"])\\\n",
    "              .setOutputCol(\"sentence_embeddings\")\n",
    "\n",
    "# Step 3: Encodes text into high-dimensional vectors using CNN - the current SentenceDetectorDLApproach uses CNN architecture\n",
    "sentenceDetector = SentenceDetectorDLApproach() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"sentences\") \\\n",
    "    .setEpochsNumber(100)\n",
    "\n",
    "use_clf_pipeline = Pipeline(stages = [document,\n",
    "                                      bert_sent,\n",
    "                                      sentenceDetector])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2d3b11-4055-4994-8deb-036ad356ac4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the training dataset to train the model\n",
    "t0 = time.time()\n",
    "pipelineModel = use_clf_pipeline.fit(trainingData)\n",
    "print(\"Training time:\", time.time()-t0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
