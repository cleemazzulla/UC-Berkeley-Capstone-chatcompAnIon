{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q13kDnOpv9zZ"
   },
   "source": [
    "# 1 Setting up Spark NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "p02ru-Md8Wm2"
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y5MpWFrTwBy-",
    "outputId": "65c15bf1-86ba-4be7-9f1c-7d169eaa0789"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# # Mount google drive\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rqgMTz4U4KRK",
    "outputId": "5494906c-0882-4c76-ddd1-dd031f710d9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-md==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.7.1/en_core_web_md-3.7.1-py3-none-any.whl (42.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m46.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /home/ubuntu/anaconda3/lib/python3.11/site-packages (from en-core-web-md==3.7.1) (3.7.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/ubuntu/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/ubuntu/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/ubuntu/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/ubuntu/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/ubuntu/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /home/ubuntu/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/ubuntu/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/ubuntu/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/ubuntu/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /home/ubuntu/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /home/ubuntu/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/ubuntu/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/ubuntu/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/ubuntu/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/ubuntu/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.10.12)\n",
      "Requirement already satisfied: jinja2 in /home/ubuntu/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.1.3)\n",
      "Requirement already satisfied: setuptools in /home/ubuntu/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (68.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ubuntu/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/ubuntu/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /home/ubuntu/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.26.4)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/ubuntu/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ubuntu/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/ubuntu/anaconda3/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/ubuntu/anaconda3/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/ubuntu/anaconda3/lib/python3.11/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /home/ubuntu/anaconda3/lib/python3.11/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ubuntu/anaconda3/lib/python3.11/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.1.3)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_md')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n",
      "Requirement already satisfied: spark-nlp in /home/ubuntu/anaconda3/lib/python3.11/site-packages (4.3.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/18 17:14:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# # Install Java\n",
    "# !apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "\n",
    "# # Download Spark\n",
    "# if os.path.isfile(\"./spark-3.5.1-bin-hadoop3.tgz\") == False:\n",
    "#     !wget -q https://downloads.apache.org/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3.tgz\n",
    "\n",
    "# # Unzip the file\n",
    "# !tar xf spark-3.5.1-bin-hadoop3.tgz\n",
    "\n",
    "# Setup environment for Spark\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = '/home/ubuntu/spark-3.5.1-bin-hadoop3'\n",
    "\n",
    "# Import findspark and load it\n",
    "!pip install -q findspark\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# import spacy for NLP and re for regular expressions\n",
    "import spacy\n",
    "spacy.cli.download(\"en_core_web_md\")\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import re\n",
    "\n",
    "# Install spark-nlp\n",
    "!pip install spark-nlp\n",
    "import sparknlp\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.common import *\n",
    "\n",
    "# Create Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder\\\n",
    "       .master(\"local\")\\\n",
    "       .appName(\"Colab\")\\\n",
    "       .config('spark.executor.memory', \"24g\")\\\n",
    "       .getOrCreate()\n",
    "\n",
    "# Start Spark Session with Spark NLP\n",
    "# spark = sparknlp.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 219
    },
    "id": "EAHj-cKh-Tu3",
    "outputId": "bfd17bde-190c-482b-eb7e-2f04619a6f5c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://06561248f69b:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark NLP</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7d1b0b79b4f0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rpU399gyzMIh"
   },
   "source": [
    "# 2 Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "twk0MhsWzQAt",
    "outputId": "946cc546-906e-471a-bd16-03db06efc0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.11/site-packages/pyspark/pandas/__init__.py:50: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "import pickle\n",
    "import urllib.request\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler, Imputer, OneHotEncoder, StringIndexer\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator, CrossValidatorModel, TrainValidationSplit, TrainValidationSplitModel\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "import pyspark.pandas as ps\n",
    "import seaborn as sns\n",
    "import altair as alt\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, fbeta_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "0myA5b5upqXV"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4531/3690270308.py:4: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import Image, display\n"
     ]
    }
   ],
   "source": [
    "# visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.display import Image, display\n",
    "import pydot\n",
    "from pydotplus import graph_from_dot_data\n",
    "from sklearn.tree import export_graphviz\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "30Id5H-8Z3tP"
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from xml.etree.ElementTree import ElementTree\n",
    "import datetime\n",
    "from pyspark.sql.functions import lit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VM49qt5rwRD5"
   },
   "source": [
    "# 3 Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kXqD4nVVEyTb"
   },
   "source": [
    "### 3.1 Obtain stratified training and test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0l0JbzHbTbqJ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/18 17:15:05 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_merged_data_conversations_path = 's3a://capstone210/data/train_merged_data_conversations/'\n",
    "test_merged_data_conversations_path = 'file:/home/ubuntu/workspace/capstone-210-spring2024/data/test'\n",
    "\n",
    "train_merged_data_conversations_df = spark.read.parquet(train_merged_data_conversations_path)\n",
    "test_merged_data_conversations_df = spark.read.parquet(test_merged_data_conversations_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "HWtPnSkKyoWj"
   },
   "outputs": [],
   "source": [
    "# train_merged_data_conversations_path = f'drive/MyDrive/210 Capstone/data/final/merged_data_conversations'\n",
    "# test_merged_data_conversations_path = f'drive/MyDrive/210 Capstone/data/final/merged_data_test'\n",
    "\n",
    "# train_merged_data_conversations_df = spark.read.parquet(train_merged_data_conversations_path)\n",
    "# test_merged_data_conversations_df = spark.read.parquet(test_merged_data_conversations_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "CEXBwf0pjAdr"
   },
   "outputs": [],
   "source": [
    "#train_df_final = df.filter(col('source') != 'PAN12-test')\n",
    "train_df_final = train_merged_data_conversations_df\n",
    "test_df_final = test_merged_data_conversations_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "KGFvX03HqcHa"
   },
   "outputs": [],
   "source": [
    "# train_df_final.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vu4OK9DXBO3X",
    "outputId": "462cb826-9c05-4fba-f03e-19d01778a8f5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+-----+-----------------------+------------------------+-----------------+--------------------+--------------------+-------+\n",
      "|     conversation_id|     source|label|conversation_start_time|n_people_in_conversation|type_conversation|         merged_text|      merged_text_id|n_texts|\n",
      "+--------------------+-----------+-----+-----------------------+------------------------+-----------------+--------------------+--------------------+-------+\n",
      "|0000604306a283600...|PAN12-train|    0|                  13:04|                       4|            Group|e3fb62ebfa4f36acf...|9fdcde97c1cb33fe4...|     67|\n",
      "|0001347c00d419eb5...|PAN12-train|    0|                  13:34|                       2|             Pair|asl say asl and i...|67952953f11f8800a...|      4|\n",
      "|000197b21283dc478...|PAN12-train|    0|                  06:27|                       2|             Pair|joint in my hand ...|487862cd4ec27d841...|     43|\n",
      "+--------------------+-----------+-----+-----------------------+------------------------+-----------------+--------------------+--------------------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df_final.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "V9uah5cHjAQt"
   },
   "outputs": [],
   "source": [
    "# train_df_final.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "yLh5pdHQjwFO"
   },
   "outputs": [],
   "source": [
    "# train_df_final.groupBy('label')\\\n",
    "#               .agg(count('*').alias('count'))\\\n",
    "#               .show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "UeCNRgMvt0f4"
   },
   "outputs": [],
   "source": [
    "# train_df_final.filter(col('label').isin([1])).show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "5BThV51RTw1H"
   },
   "outputs": [],
   "source": [
    "# test_df_final.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "tBPfF1PSTx4-"
   },
   "outputs": [],
   "source": [
    "# test_df_final.groupBy('label')\\\n",
    "#              .agg(count('*').alias('count'))\\\n",
    "#              .show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MaxRjQqC4xv4"
   },
   "source": [
    "# 4 Prepare Data before  BERT Tokenization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "36RxglM15HTc"
   },
   "source": [
    "## 4.1 Type Casting and Clean up Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JtzG5Xpvyufu",
    "outputId": "d90ae286-2431-46fd-ba34-270bda681a83"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('conversation_id', 'string'),\n",
       " ('source', 'string'),\n",
       " ('label', 'int'),\n",
       " ('conversation_start_time', 'string'),\n",
       " ('n_people_in_conversation', 'int'),\n",
       " ('type_conversation', 'string'),\n",
       " ('merged_text', 'string'),\n",
       " ('merged_text_id', 'string'),\n",
       " ('n_texts', 'int')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_final.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4MyAPx4qzQq0"
   },
   "outputs": [],
   "source": [
    "#train_df_final = train_df_final.withColumn(\"label\", col(\"label\").cast(\"integer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "S-OwBSWq444m"
   },
   "outputs": [],
   "source": [
    "# train_df_final.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Zx71zpELAAE8"
   },
   "outputs": [],
   "source": [
    "train_df_final = train_df_final.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DygOR-vDCnMf",
    "outputId": "8d16ed0b-15a7-42c6-e9e5-2493a5c77313"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    1| 2584|\n",
      "|    0|85413|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df_final.groupBy('label').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "fca9iE5GDGTs"
   },
   "outputs": [],
   "source": [
    "#train_df_final = train_df_final.filter((col('label') == 1) | (col('label') == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "a4AULVwQBfw5"
   },
   "outputs": [],
   "source": [
    "# train_df_final.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "efMTYCXJG1OI"
   },
   "outputs": [],
   "source": [
    "# train_df_final.groupBy('label').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "2CIJGWtSZuXp"
   },
   "outputs": [],
   "source": [
    "# train_df_pandas = train_df_final.select('label').toPandas()\n",
    "# ax = sns.countplot(x=\"label\", data=train_df_pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "pVzCoU8Fc7gK"
   },
   "outputs": [],
   "source": [
    "# plt.pie(train_df_pandas[\"label\"].value_counts(), labels=['normal','groomer'], autopct='%1.1f%%')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "IJxuECZYfwbu"
   },
   "outputs": [],
   "source": [
    "# function to undersample dataset automatically\n",
    "def undersample(df, outcome_col, seed=1234):\n",
    "  # Split dataset based on outcome\n",
    "  split0_df = df.filter(col(outcome_col) == 0)\n",
    "  split1_df = df.filter(col(outcome_col) == 1)\n",
    "  # determine which split is major vs minor\n",
    "  if (split0_df.count() > split1_df.count()):\n",
    "    major_df = split0_df\n",
    "    minor_df = split1_df\n",
    "  else:\n",
    "    minor_df = split0_df\n",
    "    major_df = split1_df\n",
    "  ratio = major_df.count()/minor_df.count()\n",
    "  print(\"Ratio of major vs minor before sampling: {}\".format(ratio))\n",
    "  # Start under-sampling with Spark\n",
    "  sampled_majority_df = major_df.sample(False, 1/ratio, seed)\n",
    "  combined_df = sampled_majority_df.unionAll(minor_df)\n",
    "  print(f\"Final sample size: {combined_df.count()}\")\n",
    "  return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 484
    },
    "id": "UCnsxuCPfz-s",
    "outputId": "ce9e2f91-87d7-4bb2-dd28-ab83ba9a5175"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio of major vs minor before sampling: 33.05456656346749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final sample size: 5239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Perform undersampling technique\n",
    "df_train_UnderSampled = undersample(train_df_final, outcome_col='label')\n",
    "df_pandas = df_train_UnderSampled.select('label').toPandas()\n",
    "ax = sns.countplot(x=\"label\",data=df_pandas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wszZB2cTOVHn"
   },
   "source": [
    "#### Calculate class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "fUxJPo5idlaU"
   },
   "outputs": [],
   "source": [
    "# plt.pie(df_pandas[\"label\"].value_counts(), labels=['normal','groomer'], autopct='%1.1f%%')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "rkndTqUh9M5p"
   },
   "outputs": [],
   "source": [
    "# df_train_UnderSampled.filter(col('label')==1).select(\"merged_text\").take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ab7nuZ_u3R04"
   },
   "source": [
    "### 4.2 Use Spacy English to test tokenization first and remove those samples failed to convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "KeOxS1Yz3m8V"
   },
   "outputs": [],
   "source": [
    "# Load the SpaCy en_core_web_sm model\n",
    "nlp = spacy.load('en_core_web_sm', disable=[\"tagger\", \"parser\", \"ner\"])\n",
    "\n",
    "@staticmethod\n",
    "def get_spacy():\n",
    "\n",
    "    if \"nlp\" not in globals():\n",
    "        globals()[\"nlp\"] = nlp\n",
    "\n",
    "    return globals()[\"nlp\"]\n",
    "\n",
    "# doc1 = nlp(\"i love my pet dog\")\n",
    "# doc2 = nlp(\"Maggie is my lovable pet dog!\")\n",
    "# print(doc1.vector.size)\n",
    "# print(doc2.vector.size)\n",
    "# print(\"output:\" , doc1.similarity(doc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "5ajxz9he33Kb"
   },
   "outputs": [],
   "source": [
    "# vec1 = nlp(\"i love my pet dog\").vector\n",
    "# vec1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "8KuEecZDi4lB"
   },
   "outputs": [],
   "source": [
    "# type(vec1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l1Mc_ScgxfRm"
   },
   "source": [
    "#### Build PySpark UDF to convert words into vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "hldEogxgyE0y"
   },
   "outputs": [],
   "source": [
    "nlpUDF = udf(lambda x: nlp(x).vector.tolist(), ArrayType(FloatType()))\n",
    "\n",
    "df_train_UnderSampled = df_train_UnderSampled.withColumn('array', nlpUDF(col('merged_text')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "c-eHsLyH9YDe"
   },
   "outputs": [],
   "source": [
    "arrayUDF = udf(lambda array: Vectors.dense(array), VectorUDT())\n",
    "\n",
    "df_train_UnderSampled = df_train_UnderSampled.withColumn('vector', arrayUDF(col('array')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "R72w7BWEp4Qt"
   },
   "outputs": [],
   "source": [
    "df_train_UnderSampled = df_train_UnderSampled.withColumn('array_size', size(col('array')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "38JHsUtHrM_-"
   },
   "outputs": [],
   "source": [
    "# df_train_UnderSampled.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "scEKki0PrRRf"
   },
   "outputs": [],
   "source": [
    "# df_train_UnderSampled.filter(col(\"array_size\") != 96).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rz66ChHTsk0V"
   },
   "source": [
    "#### Remove samples which failed to convert into vctors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "DFOed5LMsQEs"
   },
   "outputs": [],
   "source": [
    "df_train_UnderSampled = df_train_UnderSampled.filter(col(\"array_size\") == 96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9-pqY1oG1WNX",
    "outputId": "207971fc-aabd-44b5-8960-ed45112d568b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.11/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    0| 2655|\n",
      "|    1| 2583|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train_UnderSampled.groupBy('label').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5pG6_9C4Ucri"
   },
   "source": [
    "# 5 Model Building: Spark-NLP BERT Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iRM2vrMS6Vbg"
   },
   "source": [
    "## 5.1 Use Spark-NLP BERT Pretrained English language model to turn words into embeddings and then run binary classifier for transferred-learning/fine-tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pY3P9UXe9XLm"
   },
   "source": [
    "### Split dataset into training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xIlt5pp5P8US",
    "outputId": "b856e392-29cf-43bf-db10-203e84c0d0a9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 3644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 24:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dataset Count: 1594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dataset = df_train_UnderSampled.select(\"merged_text\",\"conversation_start_time\", \"type_conversation\", \"label\")\n",
    "(trainingData, validationData) = dataset.randomSplit([0.7, 0.3], seed = 100)\n",
    "print(\"Training Dataset Count: \" + str(trainingData.count()))\n",
    "print(\"Validation Dataset Count: \" + str(validationData.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U1_46j7ylYg6",
    "outputId": "3c52a1e5-856d-4f56-e10d-58b0d8c8fe5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------------+-----------------+-----+\n",
      "|         merged_text|conversation_start_time|type_conversation|label|\n",
      "+--------------------+-----------------------+-----------------+-----+\n",
      "| ! hi &lt;_&lt; Hi !|                  06:58|             Pair|    0|\n",
      "|!spammer pending ...|                  09:09|             Pair|    0|\n",
      "+--------------------+-----------------------+-----------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainingData.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "anRY-9Ps9fTO"
   },
   "source": [
    "### Build ML Pipeline with pretrained BERT English \"Sentence\" Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "7NkjhVeP6xHE"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.linalg import VectorUDT\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hVdAyP01gpuo"
   },
   "source": [
    "#### 5.1.A BERT-CNN with just merged_text as input feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oJyjkFHA_198",
    "outputId": "466201f0-443c-4b47-a484-7b330113d3e7"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'JavaPackage' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Step 1: Transforms raw texts to `document` annotation\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m document \u001b[38;5;241m=\u001b[39m DocumentAssembler()\\\n\u001b[1;32m      3\u001b[0m               \u001b[38;5;241m.\u001b[39msetInputCol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmerged_text\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n\u001b[1;32m      4\u001b[0m               \u001b[38;5;241m.\u001b[39msetOutputCol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocument\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n\u001b[1;32m      5\u001b[0m               \u001b[38;5;241m.\u001b[39msetCleanupMode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshrink\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Step 2: Encodes text into high dimensional vectors\u001b[39;00m\n\u001b[1;32m      8\u001b[0m bert_sent \u001b[38;5;241m=\u001b[39m BertSentenceEmbeddings\u001b[38;5;241m.\u001b[39mpretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msent_small_bert_L8_512\u001b[39m\u001b[38;5;124m'\u001b[39m)\\\n\u001b[1;32m      9\u001b[0m               \u001b[38;5;241m.\u001b[39msetInputCols([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocument\u001b[39m\u001b[38;5;124m\"\u001b[39m])\\\n\u001b[1;32m     10\u001b[0m               \u001b[38;5;241m.\u001b[39msetOutputCol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pyspark/__init__.py:139\u001b[0m, in \u001b[0;36mkeyword_only.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMethod \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m forces keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sparknlp/base/document_assembler.py:96\u001b[0m, in \u001b[0;36mDocumentAssembler.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;129m@keyword_only\u001b[39m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28msuper\u001b[39m(DocumentAssembler, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(classname\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcom.johnsnowlabs.nlp.DocumentAssembler\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setDefault(outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocument\u001b[39m\u001b[38;5;124m\"\u001b[39m, cleanupMode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisabled\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pyspark/__init__.py:139\u001b[0m, in \u001b[0;36mkeyword_only.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMethod \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m forces keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sparknlp/internal/annotator_transformer.py:36\u001b[0m, in \u001b[0;36mAnnotatorTransformer.__init__\u001b[0;34m(self, classname)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msetParams(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m_java_class_name \u001b[38;5;241m=\u001b[39m classname\n\u001b[0;32m---> 36\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_java_obj(classname, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muid)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pyspark/ml/wrapper.py:86\u001b[0m, in \u001b[0;36mJavaWrapper._new_java_obj\u001b[0;34m(java_class, *args)\u001b[0m\n\u001b[1;32m     84\u001b[0m     java_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(java_obj, name)\n\u001b[1;32m     85\u001b[0m java_args \u001b[38;5;241m=\u001b[39m [_py2java(sc, arg) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args]\n\u001b[0;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m java_obj(\u001b[38;5;241m*\u001b[39mjava_args)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'JavaPackage' object is not callable"
     ]
    }
   ],
   "source": [
    "# Step 1: Transforms raw texts to `document` annotation\n",
    "document = DocumentAssembler()\\\n",
    "              .setInputCol(\"merged_text\")\\\n",
    "              .setOutputCol(\"document\")\\\n",
    "              .setCleanupMode(\"shrink\")\n",
    "\n",
    "# Step 2: Encodes text into high dimensional vectors\n",
    "bert_sent = BertSentenceEmbeddings.pretrained('sent_small_bert_L8_512')\\\n",
    "              .setInputCols([\"document\"])\\\n",
    "              .setOutputCol(\"sentence_embeddings\")\n",
    "\n",
    "# Step 3: Define a CNN layer\n",
    "def cnn_layer(text):\n",
    "    # Your CNN implementation here using TensorFlow or any other deep learning framework\n",
    "    # This is just a placeholder example\n",
    "    # Replace it with your actual CNN model\n",
    "    cnn_model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length),\n",
    "        tf.keras.layers.Conv1D(128, 5, activation='relu'),\n",
    "        tf.keras.layers.GlobalMaxPooling1D(),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    # Assuming you have a tokenizer to tokenize text\n",
    "    tokenized_text = tokenizer(text)\n",
    "    # Convert tokenized text to tensors and pass through CNN model\n",
    "    embeddings = cnn_model.predict(tokenized_text)\n",
    "    return embeddings.tolist()\n",
    "\n",
    "# Register the function as a UDF\n",
    "cnn_layer_udf = udf(cnn_layer, ArrayType(StringType()))\n",
    "\n",
    "# Step 4: Add the CNN layer using a UDF\n",
    "data = trainingData.withColumn(\"cnn_sentence_embeddings\", cnn_layer_udf(\"merged_text\"))\n",
    "\n",
    "# Stage 4: Performs model training\n",
    "classsifierdl = ClassifierDLApproach()\\\n",
    "                  .setInputCols([\"sentence_embeddings\"])\\\n",
    "                  .setOutputCol(\"class\")\\\n",
    "                  .setLabelColumn(\"label\")\\\n",
    "                  .setMaxEpochs(20)\\\n",
    "                  .setLr(0.001)\\\n",
    "                  .setBatchSize(16)\\\n",
    "                  .setEnableOutputLogs(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uoiCjUL0HoPx"
   },
   "outputs": [],
   "source": [
    "use_clf_pipeline = Pipeline(stages = [document,\n",
    "                                      bert_sent,\n",
    "                                      classsifierdl])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1dqM5lAIg2HV",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 5.1.B BERT-CNN with merged_text, conversation_start_time, type_conversation as input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ljx4uypdg0_N",
    "outputId": "9e930aff-be9f-4096-f55a-e0ac9dc8bdd1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent_small_bert_L8_512 download started this may take some time.\n",
      "Approximate size to download 149.1 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Transforms raw texts to `document` annotation\n",
    "document = DocumentAssembler()\\\n",
    "              .setInputCol(\"merged_text\")\\\n",
    "              .setOutputCol(\"document\")\\\n",
    "              .setCleanupMode(\"shrink\")\n",
    "\n",
    "# Step 2: Encodes text into high dimensional vectors\n",
    "bert_sent = BertSentenceEmbeddings.pretrained('sent_small_bert_L8_512')\\\n",
    "              .setInputCols([\"document\"])\\\n",
    "              .setOutputCol(\"sentence_embeddings\")\n",
    "\n",
    "# Step 3: Define a CNN layer\n",
    "def cnn_layer(merged_text, conversation_start_time, type_conversation):\n",
    "    # Assuming you have a tokenizer to tokenize text\n",
    "    tokenized_text = tokenizer(text)\n",
    "\n",
    "    # Convert additional features to tensors (if needed)\n",
    "    convo_start_time_tensor = torch.tensor(conversation_start_time)\n",
    "    type_conversation_tensor = torch.tensor(type_conversation)\n",
    "\n",
    "    # cnn model\n",
    "    cnn_model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length),\n",
    "        tf.keras.layers.Conv1D(128, 5, activation='relu'),\n",
    "        tf.keras.layers.GlobalMaxPooling1D(),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    # Concatenate tokenized text tokens with additional features\n",
    "    concatenated_input = torch.cat([tokenized_text, convo_start_time_tensor, type_conversation_tensor], dim=1)\n",
    "\n",
    "    # Convert tokenized text to tensors and pass through CNN model\n",
    "    embeddings = cnn_model.predict(concatenated_input)\n",
    "\n",
    "    return embeddings.tolist()\n",
    "\n",
    "# Register the function as a UDF\n",
    "cnn_layer_udf = udf(cnn_layer, ArrayType(StringType()))\n",
    "\n",
    "# Step 4: Add the CNN layer using a UDF\n",
    "data = trainingData.withColumn(\"cnn_sentence_embeddings\", cnn_layer_udf(\"merged_text\", \"conversation_start_time\", \"type_conversation\"))\n",
    "\n",
    "# Stage 4: Performs model training\n",
    "classsifierdl = ClassifierDLApproach()\\\n",
    "                  .setInputCols([\"sentence_embeddings\"])\\\n",
    "                  .setOutputCol(\"class\")\\\n",
    "                  .setLabelColumn(\"label\")\\\n",
    "                  .setMaxEpochs(20)\\\n",
    "                  .setLr(0.001)\\\n",
    "                  .setBatchSize(16)\\\n",
    "                  .setEnableOutputLogs(True)\n",
    "\n",
    "use_clf_pipeline = Pipeline(stages = [document,\n",
    "                                      bert_sent,\n",
    "                                      classsifierdl])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "vTR5GDb6nsob"
   },
   "outputs": [],
   "source": [
    "pipelineModel = use_clf_pipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PUVzfrLCnt30",
    "outputId": "81dcf15a-eb2e-4f43-b411-947645cb0f19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.91      0.92       803\n",
      "           1       0.92      0.93      0.92       814\n",
      "\n",
      "    accuracy                           0.92      1617\n",
      "   macro avg       0.92      0.92      0.92      1617\n",
      "weighted avg       0.92      0.92      0.92      1617\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We are going to use sklearn to evalute the results on validation dataset\n",
    "\n",
    "preds = pipelineModel.transform(validationData)\n",
    "\n",
    "preds_df = preds.select('label',\"class.result\").toPandas()\n",
    "\n",
    "preds_df['result'] = preds_df['result'].apply(lambda x : int(x[0]))\n",
    "\n",
    "print (classification_report(preds_df['label'], preds_df['result']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Qc85pleUnwv9"
   },
   "outputs": [],
   "source": [
    "f_beta3_score = fbeta_score(preds_df['label'], preds_df['result'], average='binary', beta=3, pos_label=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "daON_wj2nyZE",
    "outputId": "76f63be4-5473-44ee-d487-c00a35051a65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation F-beta3 score:  0.9308314937454011\n"
     ]
    }
   ],
   "source": [
    "print(\"Validation F-beta3 score: \", f_beta3_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "CCOfRXH7n07f"
   },
   "outputs": [],
   "source": [
    "test_set = test_df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 512
    },
    "id": "B4h5-Shgn2Y4",
    "outputId": "63f62250-d542-48be-f4de-6f2652c256c8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/content/spark-3.5.1-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/content/spark-3.5.1-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-83466edaf35b>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpreds_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipelineModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mpreds_test_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreds_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"class.result\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mpreds_test_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'result'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreds_test_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'result'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/spark-3.5.1-bin-hadoop3/python/pyspark/sql/pandas/conversion.py\u001b[0m in \u001b[0;36mtoPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;31m# Below is toPandas without Arrow optimization.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m         \u001b[0mrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             pdf = pd.DataFrame.from_records(\n",
      "\u001b[0;32m/content/spark-3.5.1-bin-hadoop3/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \"\"\"\n\u001b[1;32m   1260\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1261\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1262\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/spark-3.5.1-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1322\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/content/spark-3.5.1-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/spark-3.5.1-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m                 \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m                 \u001b[0;31m# Happens when a the other end is dead. There might be an empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# We are going to use sklearn to evalute the results on test dataset\n",
    "\n",
    "preds_test = pipelineModel.transform(test_set)\n",
    "\n",
    "preds_test_df = preds_test.select('label',\"class.result\").toPandas()\n",
    "\n",
    "preds_test_df['result'] = preds_test_df['result'].apply(lambda x : int(x[0]))\n",
    "\n",
    "print (classification_report(preds_test_df['label'], preds_test_df['result']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "izu-nRxwn4SF"
   },
   "outputs": [],
   "source": [
    "f_beta3_test_score = fbeta_score(preds_test_df['label'], preds_test_df['result'], average='binary', beta=3, pos_label=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NXXqb6V0n6eF"
   },
   "outputs": [],
   "source": [
    "print(\"Test F-beta3 score: \", f_beta3_test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ftyw9NQl9o85"
   },
   "source": [
    "## 5.2 Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UXZpT5v9LR1Z"
   },
   "outputs": [],
   "source": [
    "# remove the existing logs\n",
    "# ! rm -r /root/annotator_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "suv6mQycLZAK"
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# Fit the training dataset to train the model\n",
    "pipelineModel = use_clf_pipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v_ELbGafLeNq"
   },
   "outputs": [],
   "source": [
    "# log_files = os.listdir(\"/root/annotator_logs\")\n",
    "\n",
    "# with open(\"/root/annotator_logs/\"+log_files[0], \"r\") as log_file :\n",
    "#     print(log_file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "70tEqLV8-IAJ"
   },
   "source": [
    "## 5.3 Evaluate Validation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yUbJsG53LeYa",
    "outputId": "890d4117-758c-4a0b-869b-30c2c5d94f74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.91      0.93       821\n",
      "           1       0.91      0.95      0.93       852\n",
      "\n",
      "    accuracy                           0.93      1673\n",
      "   macro avg       0.93      0.93      0.93      1673\n",
      "weighted avg       0.93      0.93      0.93      1673\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We are going to use sklearn to evalute the results on validation dataset\n",
    "\n",
    "preds = pipelineModel.transform(validationData)\n",
    "\n",
    "preds_df = preds.select('label',\"class.result\").toPandas()\n",
    "\n",
    "preds_df['result'] = preds_df['result'].apply(lambda x : int(x[0]))\n",
    "\n",
    "print (classification_report(preds_df['label'], preds_df['result']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "LB1_2i6Wqj2Q"
   },
   "outputs": [],
   "source": [
    "f_beta3_score = fbeta_score(preds_df['label'], preds_df['result'], average='binary', beta=3, pos_label=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iMenDNFKqj5w",
    "outputId": "cf814f3b-b456-42d0-b4ca-9b031aa5367a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation F-beta3 score:  0.9448082319925164\n"
     ]
    }
   ],
   "source": [
    "print(\"Validation F-beta3 score: \", f_beta3_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4AYvNOsS2Epe"
   },
   "source": [
    "## 5.4 Run against the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "e0GwGJvK2w6v"
   },
   "outputs": [],
   "source": [
    "test_set = test_df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bzTGK0bfqj9N"
   },
   "outputs": [],
   "source": [
    "# We are going to use sklearn to evalute the results on validation dataset\n",
    "\n",
    "preds_test = pipelineModel.transform(test_set)\n",
    "\n",
    "preds_test_df = preds_test.select('label',\"class.result\").toPandas()\n",
    "\n",
    "preds_test_df['result'] = preds_test_df['result'].apply(lambda x : int(x[0]))\n",
    "\n",
    "print (classification_report(preds_test_df['label'], preds_test_df['result']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9mwl1AztLecI"
   },
   "outputs": [],
   "source": [
    "f_beta3_test_score = fbeta_score(preds_test_df['label'], preds_test_df['result'], average='binary', beta=3, pos_label=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dd_CBl8bLefe"
   },
   "outputs": [],
   "source": [
    "print(\"Test F-beta3 score: \", f_beta3_test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zKGBaNKJ_2NX"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
