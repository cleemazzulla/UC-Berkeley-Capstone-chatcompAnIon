{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fcfae30-0915-4f0a-92de-cb34db40cf11",
   "metadata": {},
   "source": [
    "This notebook explores a neural network model built on top of BERT. We also conduct finetuning of the underlying bert weights in addition to the neural network weights. This model does not use any up/down sampling to address the class imbalance. Instead it uses the class weights, to intialize the bias to push the model towards favoring the minority class more. \n",
    "\n",
    "Also given the size of the model we introduce global quantization to reduce precision from 32 to 16 bit precision. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c0d42d-238f-4f61-b866-80093a2218dc",
   "metadata": {},
   "source": [
    "# Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4aff4dd-fcb9-4940-afb2-4921e1c73e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/23 16:25:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apache Spark version:  3.5.1\n",
      "Apache Spark version:  3.5.1\n"
     ]
    }
   ],
   "source": [
    "# !pip install pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "import findspark\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\" \n",
    "os.environ[\"SPARK_HOME\"] = '/home/ubuntu/spark-3.5.1-bin-hadoop3'\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"Spark\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.driver.memory\", \"120G\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "\n",
    "print(\"Apache Spark version: \", spark.version)\n",
    "print(\"Apache Spark version: \", spark.version) \n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d93480c-21d2-48fd-929a-94df4396c0e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch CUDA Available = True\n",
      "Pytorch CUDA Device Count = 4\n",
      "Pytorch CUDA Current Device = 0\n",
      "Pytorch CUDA Current Device Name = NVIDIA A10G\n"
     ]
    }
   ],
   "source": [
    "# !pip install torch\n",
    "# !pip install findspark\n",
    "import findspark\n",
    "findspark.init()\n",
    "import torch\n",
    "print(\"Pytorch CUDA Available =\", torch.cuda.is_available())\n",
    "print(\"Pytorch CUDA Device Count =\", torch.cuda.device_count())\n",
    "print(\"Pytorch CUDA Current Device =\", torch.cuda.current_device())\n",
    "print(\"Pytorch CUDA Current Device Name =\", torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b3c2a17-42f0-4c01-8d8d-5edf901a81eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q --upgrade keras-nlp\n",
    "!pip install -q --upgrade keras # Upgrade to Keras 3.\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive/')\n",
    "import tensorflow as tf\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers \n",
    "import keras_nlp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ee2af5e-35c2-4643-a9e1-e00493e3ce8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LogicalDevice(name='/device:GPU:0', device_type='GPU'), LogicalDevice(name='/device:GPU:1', device_type='GPU'), LogicalDevice(name='/device:GPU:2', device_type='GPU'), LogicalDevice(name='/device:GPU:3', device_type='GPU'), LogicalDevice(name='/device:GPU:4', device_type='GPU')]\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3', '/job:localhost/replica:0/task:0/device:GPU:4')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3', '/job:localhost/replica:0/task:0/device:GPU:4')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of devices: 5\n",
      "160\n",
      "0.0005\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3', '/job:localhost/replica:0/task:0/device:GPU:4')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3', '/job:localhost/replica:0/task:0/device:GPU:4')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of devices: 5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.mixed_precision import set_global_policy\n",
    "\n",
    "# Set TensorFlow to use TensorFlow as backend\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "# Configure the GPU memory and set logical devices before initializing them\n",
    "physical_devices = tf.config.list_physical_devices(\"GPU\")\n",
    "if physical_devices:\n",
    "    try:\n",
    "        # Assuming you have one GPU available, split it into two logical devices\n",
    "        tf.config.set_logical_device_configuration(\n",
    "            physical_devices[0],\n",
    "            [\n",
    "                tf.config.LogicalDeviceConfiguration(memory_limit=15360 // 2),\n",
    "                tf.config.LogicalDeviceConfiguration(memory_limit=15360 // 2),\n",
    "            ]\n",
    "        )\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "# Now initialize the logical devices\n",
    "logical_devices = tf.config.list_logical_devices(\"GPU\")\n",
    "print(logical_devices)\n",
    "\n",
    "base_batch_size = 32\n",
    "base_learning_rate = 1e-4\n",
    "\n",
    "# Initialize the distributed training strategy after setting up logical devices\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print(f\"Number of devices: {strategy.num_replicas_in_sync}\")\n",
    "\n",
    "scaled_batch_size = base_batch_size * strategy.num_replicas_in_sync \n",
    "print(scaled_batch_size)\n",
    "scaled_learning_rate = base_learning_rate * strategy.num_replicas_in_sync\n",
    "print(scaled_learning_rate)\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print(f\"Number of devices: {strategy.num_replicas_in_sync}\")\n",
    "\n",
    "# Set the global policy to mixed_float16\n",
    "set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afad6b63-5b13-41f4-b358-896dd52e6f9c",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e301f48c-0574-436e-a668-70f6cf11c7d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/23 16:26:16 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_merged_data_conversations_path = 's3a://capstone210/data/train_merged_data_conversations/'\n",
    "test_merged_data_conversations_path = 's3a://capstone210/data/test_merged_data_conversations/'\n",
    "\n",
    "#latest\n",
    "df_train = spark.read.parquet(train_merged_data_conversations_path)\n",
    "df_test = spark.read.parquet(test_merged_data_conversations_path)\n",
    "\n",
    "#latest\n",
    "df_train = df_train.filter((F.col('label') == 1) | (F.col('label') == 0))\n",
    "df_test = df_test.filter((F.col('label') == 1) | (F.col('label') == 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca6ca645-115b-4e41-9263-cebf22df1bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio of major vs minor before sampling: 33.05456656346749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:=============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final sample size: 5239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# ################## CLASS IMBALANCE ############################\n",
    "def undersample(df, outcome_col, seed=1234):\n",
    "  # Split dataset based on outcome\n",
    "  split0_df = df.filter(F.col(outcome_col) == 0)\n",
    "  split1_df = df.filter(F.col(outcome_col) == 1)\n",
    "  # determine which split is major vs minor\n",
    "  if (split0_df.count() > split1_df.count()):\n",
    "    major_df = split0_df\n",
    "    minor_df = split1_df\n",
    "  else:\n",
    "    minor_df = split0_df\n",
    "    major_df = split1_df\n",
    "  ratio = major_df.count()/minor_df.count()\n",
    "  print(\"Ratio of major vs minor before sampling: {}\".format(ratio))\n",
    "  # Start under-sampling with Spark\n",
    "  sampled_majority_df = major_df.sample(False, 1/ratio, seed)\n",
    "  combined_df = sampled_majority_df.unionAll(minor_df)\n",
    "  print(f\"Final sample size: {combined_df.count()}\")\n",
    "  return combined_df\n",
    "\n",
    "# Perform undersampling technique\n",
    "df_train = undersample(df_train, outcome_col='label')\n",
    "####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9f55004-901f-4b81-86f1-a964605c631e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_train = df_train.select(\"merged_text\",\"label\")\n",
    "df_test = df_test.select(\"merged_text\",\"label\")\n",
    "(df_trainsplit, df_valsplit) = df_train.randomSplit([0.7, 0.3], seed = 100)\n",
    "\n",
    "pandas_df_train = df_trainsplit.toPandas()\n",
    "pandas_df_val = df_valsplit.toPandas()\n",
    "pandas_df_test = df_test.toPandas()\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "pandas_df_train = shuffle(pandas_df_train)\n",
    "pandas_df_val = shuffle(pandas_df_val)\n",
    "pandas_df_test = shuffle(pandas_df_test)\n",
    "\n",
    "\n",
    "del df_train\n",
    "del df_valsplit\n",
    "del df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e711f94-23aa-43d4-8f74-3bd36ead382e",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca007c53-ea6f-40d4-b463-5e1e930da096",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "text_column_name = \"merged_text\"\n",
    "labels_column_name = \"label\"\n",
    "# Extract texts and labels\n",
    "train_texts = pandas_df_train[text_column_name].values\n",
    "train_labels = pandas_df_train[labels_column_name].values\n",
    "\n",
    "val_texts = pandas_df_val[text_column_name].values\n",
    "val_labels = pandas_df_val[labels_column_name].values\n",
    "\n",
    "test_texts = pandas_df_test[text_column_name].values\n",
    "test_labels = pandas_df_test[labels_column_name].values\n",
    "\n",
    "#keras expects specific dimension for binary, need to adjust the labels dim, extra dimension\n",
    "train_labels = np.expand_dims(train_labels, axis=-1)\n",
    "val_labels = np.expand_dims(val_labels, axis=-1)\n",
    "test_labels = np.expand_dims(test_labels, axis=-1)\n",
    "\n",
    "# Create TensorFlow Datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_texts, train_labels))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_texts, val_labels))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_texts, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aadd166f-b26e-4349-996e-6512a7476a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# # Assuming train_labels is a 2D numpy array as indicated by the error\n",
    "# # train_labels = pandas_df_train[labels_column_name].values\n",
    "# # Convert the 2D numpy array to 1D\n",
    "# train_labels_1d = train_labels.ravel()\n",
    "# # Convert the numpy array to a pandas Series\n",
    "# labels_series = pd.Series(train_labels_1d)\n",
    "# # Get the count of unique values\n",
    "# frequency_count = labels_series.value_counts()\n",
    "# # Display the frequency count\n",
    "# print(frequency_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "116f4226-1788-4002-9ab2-93d08c7c2578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23028 MiB\n",
      "23028 MiB\n",
      "23028 MiB\n",
      "23028 MiB\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi --query-gpu=memory.total --format=csv,noheader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b83d2f21-d686-470a-b0b9-f279b04434fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_nlp\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Initialize BERT preprocessor and backbone from a preset\n",
    "# preprocessor = keras_nlp.models.BertPreprocessor.from_preset(\"bert_tiny_en_uncased\")\n",
    "# backbone = keras_nlp.models.BertBackbone.from_preset(\"bert_tiny_en_uncased\")\n",
    "preprocessor = keras_nlp.models.BertPreprocessor.from_preset(\"bert_base_en_uncased\")\n",
    "backbone = keras_nlp.models.BertBackbone.from_preset(\"bert_base_en_uncased\")\n",
    "\n",
    "# Batch the dataset before preprocessing\n",
    "train_dataset_batched = train_dataset.batch(scaled_batch_size)\n",
    "val_dataset_batched = val_dataset.batch(scaled_batch_size)\n",
    "test_dataset_batched = test_dataset.batch(scaled_batch_size)\n",
    "\n",
    "# Apply preprocessing to the batched dataset\n",
    "train_dataset_preprocessed = (\n",
    "    train_dataset_batched.map(\n",
    "        lambda x, y: (preprocessor(x), y),\n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "    .cache()\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "val_dataset_preprocessed = (\n",
    "    val_dataset_batched.map(\n",
    "        lambda x, y: (preprocessor(x), y),\n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "    .cache()\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "test_dataset_preprocessed = (\n",
    "    test_dataset_batched.map(\n",
    "        lambda x, y: (preprocessor(x), y),\n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "    .cache()\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db6ce16d-7932-470d-904b-b54fe7bfc6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# # Assuming `train_labels` contains your training dataset labels\n",
    "# counts = np.bincount(train_labels)\n",
    "\n",
    "# # Display the number of positive samples and the percentage of positive samples\n",
    "# print(\"Number of positive samples in training data: {} ({:.2f}% of total)\".format(\n",
    "#     counts[1], 100 * float(counts[1]) / len(train_labels)))\n",
    "\n",
    "# # Calculate the weights for each class\n",
    "# weight_for_0 = 1.0 / counts[0]\n",
    "# weight_for_1 = 1.0 / counts[1]\n",
    "# pos = 2584\n",
    "# neg = 85413\n",
    "# # # # Calculate the weights for each class\n",
    "# # weight_for_0 = 1.0 / neg\n",
    "# # weight_for_1 = 1.0 / pos\n",
    "# weight_for_0 = 0.515\n",
    "# weight_for_1 = 17\n",
    "\n",
    "# # Define class weights dictionary\n",
    "# class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "# print(class_weight.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffca0a21-d929-4c17-860b-6de92180e782",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b81e447c-b486-4828-a8a0-4860406c9b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = [\n",
    "keras.metrics.BinaryCrossentropy(name='cross_entropy'),\n",
    "            keras.metrics.TruePositives(name='tp'),\n",
    "            keras.metrics.FalsePositives(name='fp'),\n",
    "            keras.metrics.TrueNegatives(name='tn'),\n",
    "            keras.metrics.FalseNegatives(name='fn'),\n",
    "            keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "            keras.metrics.Precision(name='precision'),\n",
    "            keras.metrics.Recall(name='recall'),\n",
    "            keras.metrics.AUC(name='auc')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3d21efb-e043-4bdf-aeb5-e85491305ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda_malloc_async\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, optimizers\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, SeparableConv2D\n",
    "\n",
    "os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'\n",
    "print(os.getenv('TF_GPU_ALLOCATOR'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c05aac03-ff54-4529-b79b-d7bbcc98903b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ segment_ids         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ token_ids           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bert_backbone       │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>),     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">109,482,2…</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BertBackbone</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ segment_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)]             │            │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ get_item_2          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bert_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">196,864</span> │ get_item_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_14          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │ dropout_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_15          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │ dropout_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ segment_ids         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ token_ids           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bert_backbone       │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m),     │ \u001b[38;5;34m109,482,2…\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mBertBackbone\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,      │            │ segment_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │ \u001b[38;5;34m768\u001b[0m)]             │            │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ get_item_2          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ bert_backbone[\u001b[38;5;34m1\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mGetItem\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │    \u001b[38;5;34m196,864\u001b[0m │ get_item_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_14          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m32,896\u001b[0m │ dropout_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_15          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │        \u001b[38;5;34m129\u001b[0m │ dropout_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">109,712,129</span> (418.52 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m109,712,129\u001b[0m (418.52 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">229,889</span> (898.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m229,889\u001b[0m (898.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">109,482,240</span> (417.64 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m109,482,240\u001b[0m (417.64 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make the BERT backbone not trainable to freeze its weights\n",
    "def make_model(backbone, num_classes=1, metrics=METRICS):\n",
    "  backbone.trainable = False\n",
    "\n",
    "  # Define the model architecture\n",
    "  inputs = backbone.input\n",
    "  sequence_output = backbone(inputs)[\"sequence_output\"]\n",
    "\n",
    "  # # Add additional transformer encoders\n",
    "  # for _ in range(1):\n",
    "  #     sequence_output = keras_nlp.layers.TransformerEncoder(\n",
    "  #         num_heads=2,\n",
    "  #         intermediate_dim=512,\n",
    "  #         dropout=0.1,\n",
    "  #     )(sequence_output)\n",
    "\n",
    "  # Use the [CLS] token output to classify\n",
    "  cls_output = sequence_output[:, backbone.cls_token_index, :]\n",
    "  cls_output = layers.Dense(256, activation='relu')(cls_output)\n",
    "  cls_output = layers.Dropout(0.1)(cls_output)\n",
    "  # cls_output = layers.Dense(256, activation='relu')(cls_output)\n",
    "  # cls_output = layers.Dropout(0.1)(cls_output)\n",
    "  cls_output = layers.Dense(128, activation='relu')(cls_output)\n",
    "  cls_output = layers.Dropout(0.1)(cls_output)\n",
    "  outputs = keras.layers.Dense(units = 1, activation='sigmoid')(cls_output)  # Assuming `num_classes` is defined\n",
    "\n",
    "  # Build the model\n",
    "  model = keras.Model(inputs, outputs)\n",
    "  sgd = optimizers.SGD(0.01)\n",
    "\n",
    "  # Compile the model\n",
    "  model.compile(\n",
    "      #optimizer=keras.optimizers.Adam(0.001),\n",
    "      optimizer=sgd,\n",
    "      #loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "      loss= tf.keras.losses.BinaryFocalCrossentropy(apply_class_balancing=True, gamma=2, from_logits=False),\n",
    "      metrics=METRICS\n",
    "  )\n",
    "  return model\n",
    "\n",
    "# Display the model summary\n",
    "model = make_model(backbone=backbone, num_classes=1, metrics=METRICS)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5c203e-cebd-4fb8-a313-1de95f9b8486",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define the EarlyStopping callback\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_precision', # Monitor validation loss\n",
    "    min_delta=0.001, # Minimum change to qualify as an improvement\n",
    "    patience=3, # Number of epochs with no improvement after which training\n",
    "    verbose=1,\n",
    "    mode='min', # Stop training when the quantity monitored has stopped dec\n",
    "    restore_best_weights=False # Restore model weights from the best epoch\n",
    ")\n",
    "# callbacks = keras.callbacks.ModelCheckpoint(\"childgrooming_NN_model_{epoch}.keras\", save_best_only=True)\n",
    "callbacks = keras.callbacks.ModelCheckpoint(\n",
    "    \"best_weights.weights.h5\",  # Filename for the saved weights\n",
    "    save_best_only=True,  # Save only the best model\n",
    "    save_weights_only=True,  # Save only the weights, not the full model\n",
    ")\n",
    "# Train the model\n",
    "model.fit(\n",
    "    train_dataset_preprocessed,\n",
    "    validation_data=val_dataset_preprocessed,\n",
    "    # batch_size= 1000,\n",
    "    epochs=2,  # Adjust the number of epochs according to your needs\n",
    "    #class_weight=class_weight,  # Use class weights\n",
    "    callbacks=[early_stopping, callbacks]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12d8a4ce-eb06-4ad1-aafc-94b61ab53c16",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f22447-bcee-4199-8166-331b9314219b",
   "metadata": {},
   "source": [
    "# Run Validation Set Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dcf28d-8063-4be7-8583-c6e07d37423d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict = model.predict(validation_generator, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False, verbose=0 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763b68d0-4ce1-492c-bc9e-3fd5dfc75846",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, fbeta_score, confusion_matrix\n",
    "# Predict probabilities for test data\n",
    "y_pred_prob = model.predict(val_dataset_preprocessed)\n",
    "# Convert probabilities to binary labels based on 0.5 threshold\n",
    "y_pred = np.where(y_pred_prob > 0.5, 1, 0)\n",
    "\n",
    "# Assuming test_labels are your true binary labels for the test set\n",
    "# Flatten y_pred to match the shape of test_labels if necessary\n",
    "y_pred = y_pred.flatten()\n",
    "\n",
    "# Calculate F_beta score with beta=3\n",
    "f_beta3_score_test = fbeta_score(val_labels, y_pred, average='binary', beta=3, pos_label=1)\n",
    "print(\"F1 Score (w/ Beta =3):\", round((f_beta3_score_test*100), 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846dc202-8f13-4f14-8c95-1069efacfd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e36082-24f7-4ad2-b526-aa0c69d19911",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(val_labels, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b08ede7-ee25-44fd-ad20-d3007a9c1466",
   "metadata": {},
   "source": [
    "# Run Test Set Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d33a5cbc-3aa6-41d0-bef2-d5c16b3d93b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2424/2424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m469s\u001b[0m 194ms/step\n",
      "F1 Score (w/ Beta =3): 0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import fbeta_score\n",
    "# Predict probabilities for test data\n",
    "y_pred_prob_test = model.predict(test_dataset_preprocessed)\n",
    "# Convert probabilities to binary labels based on 0.5 threshold\n",
    "y_pred_test = np.where(y_pred_prob_test > 0.5, 1, 0)\n",
    "\n",
    "# Assuming test_labels are your true binary labels for the test set\n",
    "# Flatten y_pred to match the shape of test_labels if necessary\n",
    "y_pred_test = y_pred_test.flatten()\n",
    "\n",
    "# Calculate F_beta score with beta=3\n",
    "f_beta3_score_test = fbeta_score(test_labels, y_pred_test, average='binary', beta=3, pos_label=1)\n",
    "print(\"F1 Score (w/ Beta =3):\", round((f_beta3_score_test*100), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3376f9fb-40bf-47ac-86fd-91ffc3e8b01a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.], dtype=float16)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_pred_prob_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5c1e9b35-dcec-448c-b1d7-4fe6835b9726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99    152233\n",
      "           1       0.00      0.00      0.00      2895\n",
      "\n",
      "    accuracy                           0.98    155128\n",
      "   macro avg       0.49      0.50      0.50    155128\n",
      "weighted avg       0.96      0.98      0.97    155128\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ubuntu/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ubuntu/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_labels, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c6c167-587f-464e-8d9b-e6bacc333b47",
   "metadata": {},
   "source": [
    "which was solved by using more neurons, more layers and adding more dropout. Also lowering learning rate almost always helps.\n",
    "\n",
    "* less.more layers?\n",
    "* shuffle the training data\n",
    "* try batch very small like 1-5\n",
    "* add more dropout like 0.3\n",
    "* try with leaky relu instead\n",
    "   * https://datascience.stackexchange.com/questions/39042/how-to-use-leakyrelu-as-activation-function-in-sequence-dnn-in-keraswhen-it-per\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd57e55-9534-4d27-b06f-110510880ede",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec557f4-35a2-4346-a86a-c95d190f7c60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
